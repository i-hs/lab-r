geom_point(color = rgb(0.7,0.2,0.5,0.5))+
geom_hline(yintercept = mean(heights$son),
linetype = 'dashed', color = 'darkblue')+
ggplot(data = heights, mapping = aes(x = father, y = son)) +
geom_point(color = rgb(0.7,0.2,0.5,0.5))+
geom_hline(yintercept = mean(heights$son),
linetype = 'dashed', color = 'darkblue')
# ggplot2를 이용한 그래프
library(ggplot2)
ggplot(data = heights, mapping = aes(x = father, y = son)) +
geom_point(color = rgb(0.7,0.2,0.5,0.5))+
geom_hline(yintercept = mean(heights$son),
linetype = 'dashed', color = 'darkblue')
ggplot(data = heights, mapping = aes(x = father, y = son)) +
geom_point(color = rgb(0.7,0.2,0.5,0.5))+
geom_hline(yintercept = mean(heights$son),
linetype = 'dashed', color = 'darkblue')+
geom_vline(xintercept = mean(heights$father),
linetype = 'dahsed', color = 'darkblue')
ggplot(data = heights, mapping = aes(x = father, y = son)) +
geom_point(color = rgb(0.7,0.2,0.5,0.5))+
geom_hline(yintercept = mean(heights$son),
linetype = 'dashed', color = 'darkblue')+
geom_vline(xintercept = mean(heights$father),
linetype = 'dahsed', color = 'darkblue')  +
stat_smooth(method = 'lm')
ggplot(data = heights, mapping = aes(x = father, y = son)) +
geom_point(color = rgb(0.7,0.2,0.5,0.5))+
geom_hline(yintercept = mean(heights$son),
linetype = 'dashed', color = 'darkblue')+
stat_smooth(method = 'lm')
ggplot(data = heights, mapping = aes(x = father, y = son)) +
geom_point(color = rgb(0.7,0.2,0.5,0.5))+
geom_hline(yintercept = mean(heights$son),
linetype = 'dashed', color = 'darkblue')+
geom_vline(xintercept = mean(heights$father),
linetype = 'dashed', color = 'darkblue')+
stat_smooth(method = 'lm')
ggplot(data = heights, mapping = aes(x = father, y = son)) +
geom_point(color = rgb(0.7,0.2,0.5,0.5))+
geom_hline(yintercept = mean(heights$son),
linetype = 'dashed', color = 'darkblue')+
geom_vline(xintercept = mean(heights$father),
linetype = 'dashed', color = 'darkblue') # 흐린 부분들은 잔차들의 오차범위
ggplot(data = heights, mapping = aes(x = father, y = son)) +
geom_point(color = rgb(0.7,0.2,0.5,0.5))+
geom_hline(yintercept = mean(heights$son),
linetype = 'dashed', color = 'darkblue')+
geom_vline(xintercept = mean(heights$father),
linetype = 'dashed', color = 'darkblue')+
stat_smooth(method = 'lm')  # 흐린 부분들은 잔차들의 오차범위
# 선형 회귀 모델식 : y = a + bx
# a = mean(y) - b * mean(x)
# 선형 회귀 모델식은 점 (x의평균,y의평균)을 지나가는 점이다.
# b = Cov(x,y) / Var(x)
m_x <- mean(heights$father)
m_y <- mean(heights$son)
cov_xy <- cov(heights$father, heights$son)
var_x <- var(heights$father)
b <- cov_xy/ var_x
a <- m_y-b*m_x
#Pearson's correlation Coefficient( 상관계수 )
cor(heights$father, heights$son)
# 챌린저호의 사고 조사 데이터!!
launch <- read.csv(file = 'mlwr/challenger.csv')
rm(list = ls())
# 챌린저호의 사고 조사 데이터!!
launch <- read.csv(file = 'mlwr/challenger.csv')
str(launch)
head(launch)
summary(launc)
summary(launch)
# 단순 선형 회귀(distress_ct ~ temperature)
plot(x = launch$temperature, y = launch$distress_ct)
lm_launch <- lm(formula=distress_ct ~ temperature, data = launch )
summary(lm_launch)
a <- lm_launch$coefficients[1] # 선형모델의 y절편
a
b <- lm_launch$coefficients[2] # 선형 모델의 기울기
b
abline(a= a, b= b, col= 'blue')
# 다중선형 회귀(multiple linear regression
# y ~ x1 + x2 + x3 + ...
str(launch)
lm_launch <- lm(formula = distress_ct ~ ., data= launch )
summary(lm_launch)
rm(list= ls())
insurance = read.csv('mlwr/insurance.csv')
# 데이터 확인
str(insurance)
# BMI(Body-Mass Index) =
summary(insurance)
# 데이터 확인
str(insurance)
# BMI(Body-Mass Index) =
summary(insurance)
# 종속 변수- expenses(의료비 지출)
boxplot(insurance$expenses)
hist(insurance$expenses)
# 상관 계수: cor(x, y)
cor(insurance$bmi, insurance$expenses)
# 상관 행렬 : 상관 계수들로 만든 행렬
cor(insurance[c('age', 'bmi', 'children', 'expenses')])
pairs(insurance[c('age', 'bmi', 'children', 'expenses')])
install.packages(psych)
install.packages('psych')
library(psych)
pairs.panels(insurance[c('age', 'bmi', 'children', 'expenses')])
pairs.panels(insurance)
# 다중 선형 회귀 (multiple linear regression)
# expenses ~ 나머지 모든 변수
ins_model <- lm(formula = expenses~., data = insurance)
ins_model
summary(ins_model)
# 선형 회귀 모델을 수정해서 모델 성능 향상 !!
# 나이의 비선형 항을 추가
insurance$age2 <- insurance$age^2
head(insurance$age2)
head(insurance[c('age', 'age2')])
# 수치형 변수를 이진화
# bmi 값이 30 이상이면 1, 그렇지 않으면 0으로 변환
insurance$bmi30 <- ifelse(insurance$bmi>30, 1, 0)
head(insurance$bmi30)
head(insurance[c('bmi','bmi30')])
# 두 변수 이상의 상호작용을 선형 회귀 모델에 추가
# 흡연 + 비만
structure(insurance)
ins_model2 <- lm(formula = expenses~ age + sex + bmi +
chileren + smoker + region+ age2 + bmi30+
smoker*bmi30,
data = insurance)
ins_model2 <- lm(formula = expenses~ age + sex + bmi +
children + smoker + region+ age2 + bmi30+
smoker*bmi30,
data = insurance)
summary(ins_model2)
rm(list= ls())
insurance = read.csv('mlwr/insurance.csv')
rm(list = ls())
# 데이터 준비
heights <- read.csv('mlwr/heights.csv')
head(heights)
head(heights)
# 아버지 키(father)의 분포
summary(heights)
hist(heights$father)
boxplot(heights$father)
# 산점도 그래프(scatter plot)
plot(heights, col = rgb(0.7,0.2,0.7,0.9))
abline(h = mean(heights$son)) #h = horizontal 가로 보조선
# 산점도 그래프(scatter plot)
plot(heights, col = rgb(0.7,0.2,0.7,0.9))
#abline(): 보조선, h : 수평보조선, v : 수직 보조선
abline(v = mean(heights$father),lty = 2)
#lm() 함수 : linear regression model(선형 회귀 모델)
lm_heights <- lm(formula = son ~ father, data = heights) # formula = 종속변수 ~ 독립변수
summary(lm_heights)
# 선형 모델에서 찾은 coefficient(계수)들을 이용해서
# 선형 모델 그래프를 추가
abline(a = 86.10257, b = 0.51391)
# ggplot2를 이용한 그래프
library(ggplot2)
ggplot(data = heights, mapping = aes(x = father, y = son)) +
geom_point(color = rgb(0.7,0.2,0.5,0.5))+
geom_hline(yintercept = mean(heights$son),
linetype = 'dashed', color = 'darkblue')+
geom_vline(xintercept = mean(heights$father),
linetype = 'dashed', color = 'darkblue')+
stat_smooth(method = 'lm')  # 흐린 부분들은 잔차들의 오차범위
insurance = read.csv('mlwr/insurance.csv')
rm(list= ls())
insurance = read.csv('mlwr/insurance.csv')
# 다중 선형 회귀 (multiple linear regression)
# expenses ~ 나머지 모든 변수
ins_model <- lm(formula = expenses~., data = insurance)
# 수치형 변수를 이진화
# bmi 값이 30 이상이면 1, 그렇지 않으면 0으로 변환
insurance$bmi30 <- ifelse(insurance$bmi>30, 1, 0)
ins_model2 <- lm(formula = expenses~ age + sex + bmi +
children + smoker + region+ age2 + bmi30+
smoker*bmi30,
data = insurance)
# 상관 행렬 : 상관 계수들로 만든 행렬
cor(insurance[c('age', 'bmi', 'children', 'expenses')])
pairs(insurance[c('age', 'bmi', 'children', 'expenses')])
# 다중 선형 회귀 (multiple linear regression)
# expenses ~ 나머지 모든 변수
ins_model <- lm(formula = expenses~., data = insurance)
ins_model
summary(ins_model)
head(insurance[c('age', 'age2')])
# 1. 데이터 준비
groceries <- read.csv(file = 'mlwr\groceries.csv')
str(groceries)
# 1. 데이터 준비
groceries <- read.csv(file = 'mlwr\groceries.csv')
# 1. 데이터 준비
groceries <- read.csv(file = 'mlwr/groceries.csv')
str(groceries)
head(groceries)
head(groceries, n = 10)
# arules 패키지: association rules(연관 규칙) 패키지
install.packages('arules')
library(arules)
# 장바구니 영수증 데이터(csv)를 희소 행렬로 만듦 !
groceries <- read.transactions(file = 'mlwr/groceries.csv',
header = F,
sep = ',')
summary(groceries)
head(groceries)
View(groceries)
inspect(groceries)
summary(groceries)
head(groceries)
inspect(groceries)
summary(groceries)
head(groceries)
inspect(groceries[1:5])
# 영수증에 등장하는 아이템들의 빈도(frequency)
itemFrequency(groceries[1:5])
# 영수증에 등장하는 아이템들의 빈도(frequency)
itemFrequency(groceries[,1:5])
# 영수증에 등장하는 아이템들의 빈도(frequency)
itemFrequency(groceries[,165:169])
source('C:/dev/lab-r/ml11.r', encoding = 'UTF-8', echo=TRUE)
# 아이템들의 빈오 분포
itemFrequencyPlot(grocefies, support = 0.1 )
# 아이템들의 빈도 분포
itemFrequencyPlot(grocefies, support = 0.1 )
# 아이템들의 빈도 분포
itemFrequencyPlot(groceries, support = 0.1 )
# support 영수증에 아이템이 나타나는 횟수 : 0.1 은 최소 10% 이상 등장하는 아이템만 포함
itemFrequencyPlot(groceries, topN = 20)
# 희소 행렬(Sparse Matrix)를 그래프로 표시
image(groceries[1:100 ])
# 3. 모델 학습 - 자율(비지도) 학습의 한 종류 a priori 알고리즘
grocery_rules <- apriori(data = groceries)
summary(grocery_rules)
# 아이템들의 빈도 분포
itemFrequencyPlot(groceries, support = 0.1 )
# support 영수증에 아이템이 나타나는 횟수 : 0.1 은 최소 10% 이상 등장하는 아이템만 포함
itemFrequencyPlot(groceries, topN = 20)
grocery_rules2 <- apriori(data = groceries,
parameter = list(support = 0.03,
confidence = 0.25,
minlen = 2))
summary(grocery_rules2)
inspect(grocery_rules2)
9835*0.003
9835*0.03
300/9835
inspect(grocery_rules2[1:5])
inspect(sort(grocery_rules2, by = 'lift'))
inspect(sort(grocery_rules2, by = 'lift')[1:10])
lift(x->y) = confidence(x->y) / support(y)
rm(list = lm())
rm(list = ls())
#Clustering (군집화)
teens <- read.csv(file='mlwr/snsdata.csv')
# 데이터 확인
str(teens)
head(teens,n = 20)
# 몇가지 변수(특징)에서 결측치(NA)가 보임!
summary(teens)
# gender 변수의 NA 갯수
table(is.na(teens$gender))
table(teens$gender, useNA = 'ifany')
# female 변수를 데이터프레임에 추가
# 성별이 'F'이고, NA가 아니면 1, 그렇지 않으면 0을 입력 - 더미코딩
teens$female <- ifelse(teens$gender == 'F' & !is.na(teens$gender),1,0)
table(teens$female, useNA = 'ifany')
# nogender 변수를 데이터프레임에 추가
# gender 변수가 NA이면 1, 그렇지 않으면 0을 입력
teens$nogender <- ifelse(is.na(teens$gender),1,0)
table(teens$nogender, useNA = 'ifany')
table(teens$nogender)
table(teens$female, useNA = 'ifany')
# age 변수 확인
summary(teens$age)
# age의 정상 범위는 13 ~ 19라고 가정 -> 이외의 값들은 NA
teens$age <- ifelse(teens$age >= 13 & teens$age <= 19, teens$age, NA)
summary(teens$age)
# age의 NA 값들을 gradeyear별 age의 평균값으로 대체
# dplyr 패키지 이용
library(dplyr)
teens %>%
group_by(gradyear) %>%
filter(!is.na(age)) %>%
summarise(mean(age))
# 그룹별 평균( 또는 임의함수 )를 적용해서 벡터를 리턴하는 함수
# stats::ave(평균을 계산할 벡터, 그룹핑 변수, FUN = mean)
df <- data.frame(class = c(1, 1, 1, 2, 2),
score = c(10, 9, NA, 9, 8))
# 그룹별 평균( 또는 임의함수 )를 적용해서 벡터를 리턴하는 함수
# stats::ave(평균을 계산할 벡터, 그룹핑 변수, FUN = mean)
df <- data.frame(class = c(1, 1, 1, 2, 2),
score = c(10, 9, 8, 9, 8))
ave(df$score, df$class, FUN = mean)
mean(x, na.rm = T)
my_mean <- function(x){
mean(x, na.rm = T)
}
ave(df$score, df$class, FUN = my_mean)
# 그룹별 평균( 또는 임의함수 )를 적용해서 벡터를 리턴하는 함수
# stats::ave(평균을 계산할 벡터, 그룹핑 변수, FUN = mean)
df <- data.frame(class = c(1, 1, 1, 2, 2),
score = c(10, 9, NA, 9, 8))
ave(df$score, df$class, FUN = mean)
ave(df$score, df$class, FUN = my_mean)
ave_age <- ave(teens$age, teens$gradyear, FUN = my_mean())
ave_age <- ave(teens$age, teens$gradyear, FUN = my_mean())
ave_age <- ave(teens$age, teens$gradyear, FUN = my_mean)
ave_age
tail(ave_age)
teens$age <- ifelse(is.na(teens$age),ave_age,teens$age)
summary(teens$age)
df
ave(df$score, df$class, FUN = mean)
ave(df$score, df$class, FUN = mean)
ave(df$score, df$class, FUN = my_mean)
ave_age
summary(teens$age)
# 개인 식별 정보 (gradyear, gender, age, friends)를 제외하고,
# 오로지 관심사들로만 clustering을 시도
interests <- teens[5:40]
str(interests)
set.seed(2345)
teen_clusters <- kmeans(interests, 5)
plot(teen_clusters)
str(teen_clusters)
str(teen_clusters$cluster)
table(teen_clusters$cluster)
# 모델이 분류한 클러스터가 어떤 특징들을 갖고 있을까?
teens$cluster <- teen_clusters$cluster
teens[c('cluster', 'gender', 'age', 'friends')]
teens[1:10, c('cluster', 'gender', 'age', 'friends')]
teen_clusters$centers
#정규화
normalize<- function(x){
return((x-min(x))/(max(x)-min(x)))
}
interests_n<-as.data.frame(lapply(interests,normalize))
str(_n)
str(interests_n)
set.seed(2345)
teen_clusters_n <- kmeans(interests_n, 5)
str(teen_clusters_n)
str(teen_clusters_n$cluster)
table(teen_clusters_n$cluster)
# 모델이 분류한 클러스터가 어떤 특징들을 갖고 있을까?
teens$cluster_n <- teen_clusters_n$cluster
teens[1:10, c('cluster_n', 'gender', 'age', 'friends')]
teen_clusters_n$centers
sms_result <- read.csv(file = "mlwr/sms_results.csv")
sms_results <- read.csv(file = "mlwr/sms_results.csv")
head(sms_results)
rm(list = ls())
sms_results <- read.csv(file = "mlwr/sms_results.csv")
head(sms_results)
# Naive Bayes 알고리즘에서 스팸 메세지 분류 결과/ 확률 정리
sms_results <- read.csv(file = 'mlwr/sms_results.csv')
library(dplyr)
sms_results %>%
filter(prob_spam> 0.4 & prob_spam < 0.6 ) %>%
head(n = 10)
sms_results %>%
filter(prob_spam> 0.4 & prob_spam < 0.6 )
# 실제 값이 예측 값과 다른 경우
sms_resulzt %>%
filter (auctual_type != predict_type) %>%
head(n = 10)
# 실제 값이 예측 값과 다른 경우
sms_resulzt %>%
filter (auctual_type != predict_type) %>%
head(n = 10)
# 실제 값이 예측 값과 다른 경우
sms_resulzt %>%
filter (auctual_type != predict_type) %>%
tail(n = 10)
# 실제 값이 예측 값과 다른 경우
sms_result %>%
filter (auctual_type != predict_type) %>%
tail(n = 10)
# 실제 값이 예측 값과 다른 경우
sms_results %>%
filter (auctual_type != predict_type) %>%
tail(n = 10)
# 실제 값이 예측 값과 다른 경우
sms_results %>%
filter (actual_type != predict_type) %>%
tail(n = 10)
# 혼동 행렬(confusion matrix)
table(sms_results$actual_type, sms_results$predict_type)
#sam/ham 분류에서 예측 확률이 50% 근처인 경우 - 예측하기 애매한 경우
# 모델에서 잘못 예측할 가능성이 크다.
sms_results %>%
filter(prob_spam> 0.4 & prob_spam < 0.6 )
# 혼동 행렬(confusion matrix)
table(sms_results$actual_type, sms_results$predict_type)
library(gmodels)
CrossTable(sms_results$actual_type, sms_results$predict_type)
# kappa 통계량 계산
# Pr(a): 실제 일치(actual agreement) 비율
# TN + TP
pr_a <- 0.865+0.109
# Pr(e): 예상 일치(expected agreement) 비율
# 독립 사건이라는 가정에서
# P(실제 스팸|스팸 예측) + P(실제 햄|햄 예측)
pr_e = (0.022+0.109)*(0.003+0.109)+(0.865+0.003)*(0.865+0.022)
kappa <- (pr_a-pr_e)/(1-pr_e)
# Pr(e): 예상 일치(expected agreement) 비율
# 독립 사건이라는 가정에서
# P(실제 스팸|스팸 예측) + P(실제 햄|햄 예측)
pr_e = (0.132)*(0.112)+(0.868)*(0.888)
kappa <- (pr_a-pr_e)/(1-pr_e)
install.packages('caret')
library(caret)
cconfusionMatrix(sms_results$actual_type, sms_results$predict_type,
positive = 'spam')
confusionMatrix(sms_results$actual_type, sms_results$predict_type,
positive = 'spam')
CrossTable(sms_results$actual_type, sms_results$predict_type,
positive = 'spam')
CrossTable(sms_results$predict_type, sms_results$actual_type,
positive = 'spam')
confusionMatrix(sms_results$predict_type, sms_results$actual_type,
positive = 'spam')
confusionMatrix(data = sms_results$predict_type, reference = sms_results$actual_type,
positive = 'spam')
confusionMatrix(data = sms_results$predict_type, reference = sms_results$actual_type,
positive = 'spam')
# data = 예측 결과, reference = 실제 결과
# positive = 관심 클래스
CrossTable(sms_results$actual_type, sms_results$predict_type,
positive = 'spam')
sensitivity(data = sms_results$predict_type,
reference = sms_results$actual_type,
positive = 'spam')
# 특이도
specificity(data = sms_results$predict_type,
reference = sms_results$actual_type,
negative = 'ham')
precision(data = sms_results$predict_type,
reference = sms_results$actual_type,
relevant = 'spam')
# 정밀도
precision(data = sms_results$predict_type,
reference = sms_results$actual_type,
relevant = 'spam')
# F - 척도
F_meas(data = sms_results$predict_type,
reference = sms_results$actual_type,
relev='spam')
f <- (2 * 0.974359 * 0.8306011) / (0.974359 + 0.8306011)
# ROC(Receiver Operation Characteristic) 곡선
install.packages('pROC')
library(pROC)
sms_roc <- roc(sms_results$actual_type, sms_results$predict_type)
sms_roc <- roc(response = sms_results$actual_type,
predictor = sms_results$prob_spam)
plot(sms_roc)
plot(sms_roc, col = 'blue')
plot(sms_roc, col = 'blue', lwd = 2)
plot(sms_roc, col = 'blue', lwd = 1)
plot(sms_roc, col = 'blue', lwd = 3)
plot(sms_roc, col = 'blue', lwd = 10)
plot(sms_roc, col = 'blue', lwd = 100)
plot(sms_roc, col = 'blue', lwd = 5)
plot(sms_roc, col = 'pink', lwd = 500)
plot(sms_roc, col = 'blue', lwd = 5)
sms_knn <- read.csv(file= 'mlwr/sms_results_knn.csv')
head(sms_knn)
sms_knn_roc <- roc(response = sms_results$actual_type,
predictor = sms_knn$p_spam)
plot(sms_knn_roc)
plot(sms_roc, col = 'blue', lwd = 3)
plot(sms_knn_roc, col = 'red', lwd = 3, add = T)
plot(sms_roc, col = 'blue', lwd = 3)
# 모델 성능 개선
rm(list = ls())
# 데이터 준비 !!
credit <- read.csv(file = 'mlwr/credit.csv')
str(credit)
library(caret)
modelLookup('C5.0')
modelLookup('knn')
set.seed(1021)
m <- train(default ~.,)
m <- train(default ~.,data = credit, method = 'C5.0')
str(m)
m
p <- pred(m, credit)
p <- predict(m, credit)
table(p, credit$default)
# 튜닝 절차 자동화
ctrl <- trainControl(method = 'cv',number = 10, # 10-fold 교차 검증
selectionFunction = 'oneSE')
ctrl
grid <- expand.grid(model = 'tree',
trials = c(1, 5, 10, 15, 20, 25, 30, 35),
winnow = F )
grid
grid2 <- expand.grid(model = c('tree', 'rules'),
trials = c(1, 5, 10, 15, 20, 25),
winnow = F)
grid2
set.seed(1021)
m <- train(default ~ ., data = credit, method = 'C5.0',
metric = 'Kappa',
trControl = ctrl,
tuneGrid = grid)
m
